{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53432ef5",
   "metadata": {},
   "source": [
    "# Connecting PG RDS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54534aad",
   "metadata": {},
   "source": [
    "### Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff923626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, rand, round, current_date\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "import random\n",
    "import adlfs\n",
    "import pandas\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aef9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "AZURE_STORAGE_ACCOUNT_NAME = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "AZURE_ACCOUNT_KEY = os.getenv(\"AZURE_ACCOUNT_KEY\")\n",
    "AZURE_CONTAINER_NAME = os.getenv(\"AZURE_CONTAINER_NAME\")\n",
    "\n",
    "RDS_HOST = os.getenv(\"RDS_HOST\")\n",
    "RDS_PORT = os.getenv(\"RDS_PORT\")\n",
    "RDS_DB = os.getenv(\"RDS_DB\")\n",
    "RDS_USER = os.getenv(\"RDS_USER\")\n",
    "RDS_PASSWORD = os.getenv(\"RDS_PASSWORD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd28108",
   "metadata": {},
   "source": [
    "### Testing Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "304b2b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=RDS_HOST,\n",
    "    database=RDS_DB,\n",
    "    user=RDS_USER,\n",
    "    password=RDS_PASSWORD,\n",
    "    port=RDS_PORT\n",
    ")\n",
    "print(\"Connection successful!\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa75ea",
   "metadata": {},
   "source": [
    "### Synthatic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abe24bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 11:23:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+--------+-----+----------+-------+\n",
      "|order_id|customer_id|product_id|quantity|price|order_date|country|\n",
      "+--------+-----------+----------+--------+-----+----------+-------+\n",
      "|       0|       2116|       262|       2|98.74|2024-12-29|    USA|\n",
      "|       1|       7478|       619|       3| 5.16|2024-08-21|    USA|\n",
      "|       2|       3043|        19|       7|10.14|2024-12-30|     UK|\n",
      "|       3|       8162|       268|       4|76.05|2024-12-18|    USA|\n",
      "|       4|       2164|       707|      10|12.82|2024-09-08|     UK|\n",
      "+--------+-----------+----------+--------+-----+----------+-------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"GenerateEcommerceData\").config(\"spark.jars\", f\"{os.getcwd()}/postgresql-42.7.5.jar\").getOrCreate()\n",
    "\n",
    "# Generate base DataFrame\n",
    "df = spark.range(100000).withColumnRenamed(\"id\", \"order_id\")\n",
    "\n",
    "# Add columns\n",
    "df = df.withColumn(\"customer_id\", (rand() * 10000).cast(IntegerType())) \\\n",
    "       .withColumn(\"product_id\", (rand() * 1000).cast(IntegerType())) \\\n",
    "       .withColumn(\"quantity\", (rand() * 10 + 1).cast(IntegerType())) \\\n",
    "       .withColumn(\"price\", round(rand() * 100 + 5, 2)) \\\n",
    "       .withColumn(\"order_date\", expr(\"current_date() - cast(rand() * 365 as int)\")) \\\n",
    "       .withColumn(\"country\", expr(\"CASE WHEN rand() < 0.5 THEN 'USA' ELSE 'UK' END\"))\n",
    "\n",
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5848ee",
   "metadata": {},
   "source": [
    "### Connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65b2c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "jdbc_url = f\"jdbc:postgresql://{RDS_HOST}:{RDS_PORT}/{RDS_DB}\"\n",
    "properties = {\n",
    "    \"user\": RDS_USER,\n",
    "    \"password\": RDS_PASSWORD,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Write synthetic data to RDS\n",
    "df.write.jdbc(url=jdbc_url, table=\"orders\", mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee4119",
   "metadata": {},
   "source": [
    "### read from data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bce33a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+--------+-----+----------+-------+\n",
      "|order_id|customer_id|product_id|quantity|price|order_date|country|\n",
      "+--------+-----------+----------+--------+-----+----------+-------+\n",
      "|   41666|       5354|       490|       8|28.36|2024-11-30|     UK|\n",
      "|   41667|       7446|       505|       3| 31.9|2025-05-15|     UK|\n",
      "|   41668|       8081|       630|       7|52.06|2024-07-24|    USA|\n",
      "|   41669|       7174|       890|       8|71.57|2025-04-23|    USA|\n",
      "|   41670|       4521|       119|       7|17.21|2024-09-16|     UK|\n",
      "+--------+-----------+----------+--------+-----+----------+-------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df = spark.read.jdbc(\n",
    "        url=f\"jdbc:postgresql://{RDS_HOST}:{RDS_PORT}/{RDS_DB}\",\n",
    "        table=\"orders\",\n",
    "        properties={\n",
    "            \"user\": RDS_USER,\n",
    "            \"password\": RDS_PASSWORD,\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    ")\n",
    "\n",
    "orders_df.show(5) \n",
    "orders_df.count() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c2af5a",
   "metadata": {},
   "source": [
    "### Connecting with adfs or data lake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc43733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded /tmp/test_output.csv to output/test_output.csv\n",
      "Verified: output/test_output.csv exists in Azure storage.\n",
      "Deleted output/test_output.csv from Azure storage.\n"
     ]
    }
   ],
   "source": [
    "storage_account_name = AZURE_STORAGE_ACCOUNT_NAME \n",
    "container_name = AZURE_CONTAINER_NAME\n",
    "account_key = AZURE_ACCOUNT_KEY\n",
    "fs = adlfs.AzureBlobFileSystem(account_name=storage_account_name, account_key=account_key)\n",
    "\n",
    "test_output_path = \"/tmp/test_output.csv\"\n",
    "orders_df.limit(10).toPandas().to_csv(test_output_path, index=False)\n",
    "\n",
    "remote_path = f\"{container_name}/test_output.csv\"\n",
    "fs.put(test_output_path, remote_path)\n",
    "print(f\"Uploaded {test_output_path} to {remote_path}\")\n",
    "\n",
    "if fs.exists(remote_path):\n",
    "    print(f\"Verified: {remote_path} exists in Azure storage.\")\n",
    "\n",
    "fs.rm(remote_path)\n",
    "print(f\"Deleted {remote_path} from Azure storage.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
